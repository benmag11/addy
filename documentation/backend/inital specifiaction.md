# Comprehensive Backend Architecture for an Exam Question Database

---

## Section 1: The Foundational Data Architecture: A Relational Approach in Supabase

The cornerstone of a scalable, performant, and maintainable application is its data architecture. The initial proposal of using large JSON files, while seemingly simple for data ingestion, presents significant long-term challenges in querying, filtering, and data integrity. A far superior approach, and the one detailed here, is to leverage the full power of Supabase's underlying PostgreSQL database by implementing a robust relational data model. This architectural decision is pivotal; it transforms complex application-layer data manipulation into highly efficient, standard database operations, enabling all requested features from filtering to performance at scale.

A query to find questions from multiple years, across specific topics, and containing certain keywords would require loading and programmatically iterating through massive JSON objects in the application layer—an inherently slow and inefficient process. By modeling the data relationally, this same query becomes a single, declarative SQL statement that the database can optimize and execute in milliseconds, leveraging the indexing strategies discussed later in this report. This relational foundation is not merely a preference but a prerequisite for building the clean and simple architecture required.

### 1.1 Core Entities and Primary Tables

The system's data can be logically decomposed into three fundamental entities, each represented by a primary table in the database. This process of normalization eliminates data redundancy and establishes clear relationships between different pieces of information.¹

* **`subjects` Table:** This table will serve as a canonical list of all exam subjects and their corresponding levels. It will store entries such as (`Mathematics`, `Higher`), (`English`, `Ordinary`), etc. By centralizing this information, it ensures consistency across the application and simplifies the user's initial selection process.
* **`topics` Table:** This table will house the predefined, curated list of all possible topics, such as 'Algebra', 'Calculus', or 'Trigonometry'. This allows topics to be managed independently and provides a stable set of foreign keys for linking questions to their relevant subject areas.
* **`questions` Table:** This is the central and most critical table in the architecture. A single row in this table will represent a single exam question and will contain all associated metadata. This includes the exam year, the question number (e.g., Q1, Q2), the full extracted text content for searching, and the structured data necessary for the keyword highlighting feature. It will also hold the URLs for the question and marking scheme images, ensuring they are treated as a single, indivisible unit.

### 1.2 The Many-to-Many Relationship: Connecting Questions and Topics

A common requirement in this domain is that a single question may cover multiple topics. For instance, a question might test concepts from both 'Algebra' and 'Geometry'. Storing a list or array of topics within the `questions` table is an anti-pattern that complicates querying and violates relational database principles. The correct and standard solution is to implement a many-to-many relationship using a "join table".¹

* **`question_topics` Join Table:** This simple yet powerful table will have only two essential columns: `question_id` and `topic_id`. Each column will be a foreign key referencing the `questions` table and the `topics` table, respectively. A row in this table creates a direct link between one question and one topic. If a question belongs to three topics, there will be three corresponding rows in this table. This design is highly efficient and allows for powerful relational queries, such as "find all topics associated with questions from 2024" or, more critically for the application, "find all questions associated with the 'Algebra' topic".⁴

### 1.3 A Unified ID Strategy: Decoupling from Filenames

The initial strategy of using filenames like `p-idnum.webp` as identifiers tightly couples the application logic to the storage layer, making the system brittle and difficult to maintain. A more robust architecture mandates that the database be the single source of truth for identity.

To achieve this, all primary keys in the database will be of the `UUID` type. PostgreSQL has native support for UUIDs, and they are a common best practice for ensuring globally unique, non-sequential identifiers for database records.¹ The `id` column of each table (`subjects`, `topics`, `questions`) will be a UUID generated by the database upon insertion. The URLs to the image files in Supabase Storage will be stored as simple `text` fields within the `questions` table, but the canonical identifier for any given question entity within the entire system will be its UUID from the `questions.id` column.

### 1.4 Atomic Data Units: Guaranteeing Question and Marking Scheme Cohesion

A critical user requirement is that a question and its corresponding marking scheme must always be fetched and displayed together. The proposed schema design enforces this at an atomic level. The `questions` table will contain two distinct text columns:
* `question_image_url`
* `marking_scheme_image_url`

By storing both URLs within the same row of the `questions` table, they become an inseparable part of a single record. Any query that retrieves a question record will, by definition, also retrieve the URLs for both the question image and its marking scheme. This elegant design completely eliminates the possibility of mismatched or missing pairs, fulfilling the requirement through the fundamental structure of the data model itself.

### 1.5 Data for Keyword Highlighting: Storing Bounding Box Coordinates

To enable the feature of highlighting search terms on the question image, the frontend needs to know the precise location of each word. This data will be generated during the offline ingestion process (detailed in Section 2) and stored directly with the question.

The `questions` table will include a column named `word_coordinates` of the type `JSONB`. PostgreSQL's `JSONB` type is a highly efficient binary format for storing and querying JSON data. During the OCR phase, tools can extract not only the text but also the bounding box coordinates (e.g., x, y, width, height) for every word on the page.⁷ This structured data will be stored in the `word_coordinates` column as a JSON object. When a user performs a keyword search, the backend will return the full question record. The frontend application can then parse the `word_coordinates` JSON to find the location of the searched keyword and dynamically render a highlight overlay on top of the displayed question image.

#### Table 1: Proposed Database Schema (Entity Relationship Diagram)

| Table           | Column                      | Type    | Key/Reference                      | Description                                           |
|-----------------|-----------------------------|---------|------------------------------------|-------------------------------------------------------|
| **subjects** | `id`                        | `uuid`  | Primary Key                        | Unique identifier for the subject-level combination.  |
|                 | `name`                      | `text`  |                                    | The name of the subject (e.g., 'Mathematics').        |
|                 | `level`                     | `text`  |                                    | The level of the subject (e.g., 'Higher').            |
| **topics** | `id`                        | `uuid`  | Primary Key                        | Unique identifier for the topic.                      |
|                 | `name`                      | `text`  | Unique                             | The name of the topic (e.g., 'Algebra').              |
|                 | `subject_id`                | `uuid`  | Foreign Key -> `subjects.id`       | Links the topic to a specific subject.                |
| **questions** | `id`                        | `uuid`  | Primary Key                        | Unique identifier for the question.                   |
|                 | `subject_id`                | `uuid`  | Foreign Key -> `subjects.id`       | Links the question to its subject and level.          |
|                 | `year`                      | `integer`|                                    | The year the exam paper was published.                |
|                 | `paper_number`              | `integer`|                                    | The paper number (e.g., 1 or 2).                      |
|                 | `question_number`           | `text`  |                                    | The question identifier (e.g., 'Q1', 'Q6a').          |
|                 | `full_text`                 | `text`  |                                    | The complete OCR-extracted text of the question.      |
|                 | `word_coordinates`          | `jsonb` |                                    | JSON object with bounding box data for each word.     |
|                 | `question_image_url`        | `text`  |                                    | Public URL to the question image in Supabase Storage. |
|                 | `marking_scheme_image_url`  | `text`  |                                    | Public URL to the marking scheme image.               |
| **question_topics**| `question_id`            | `uuid`  | Primary Key, Foreign Key -> `questions.id` | Links to a specific question.                      |
|                 | `topic_id`                  | `uuid`  | Primary Key, Foreign Key -> `topics.id`    | Links to a specific topic.                         |


#### Table 2: Detailed Column Specification for `questions` Table

| Column Name                | PostgreSQL Data Type | Constraints                               | Description                                                                                               |
|----------------------------|----------------------|-------------------------------------------|-----------------------------------------------------------------------------------------------------------|
| `id`                       | `uuid`               | Primary Key, default `uuid_generate_v4()` | The unique identifier for the question record.                                                            |
| `subject_id`               | `uuid`               | Not Null, Foreign Key (`subjects.id`)     | A reference to the subject and level this question belongs to.                                            |
| `year`                     | `integer`            | Not Null                                  | The year of the exam (e.g., 2024).                                                                        |
| `paper_number`             | `integer`            |                                           | The paper number if applicable (e.g., 1 or 2). Can be null.                                               |
| `question_number`          | `text`               | Not Null                                  | The official question number from the paper (e.g., 'Q3').                                                 |
| `full_text`                | `text`               |                                           | The complete, searchable text extracted from the question image via OCR.                                  |
| `fts_vector`               | `tsvector`           | Generated Column                          | A generated `tsvector` column based on `full_text` for optimized full-text search.                        |
| `word_coordinates`         | `jsonb`              |                                           | A JSON object containing an array of words and their corresponding bounding box coordinates on the image. |
| `question_image_url`       | `text`               | Not Null, Unique                          | The public URL of the question image stored in Supabase Storage.                                          |
| `marking_scheme_image_url` | `text`               | Not Null, Unique                          | The public URL of the associated marking scheme image stored in Supabase Storage.                         |
| `created_at`               | `timestamptz`        | `default now()`                           | Timestamp of when the record was created.                                                                 |

---

## Section 2: The Data Ingestion and Enrichment Pipeline

The transformation of raw PDF files into a structured, queryable, and enriched dataset is a complex process involving multiple stages. To ensure the live application remains fast and responsive, this entire process must be designed as an automated, offline pipeline. This pipeline will run in a secure server environment, completely decoupled from the user-facing application. This separation is a crucial architectural principle: it allows for the use of computationally intensive and highly accurate tools for data processing, as the time cost is incurred once, upfront, rather than during a live user session. The result is a higher-quality dataset that directly enables a superior user experience with features like accurate topic filtering and keyword highlighting.

### 2.1 Phase 1: Image and Metadata Generation

The first phase of the pipeline focuses on converting the source material into web-optimized assets and preparing them for storage.

* **PDF to Image Conversion:** Each individual PDF segment, representing either a single question or a single marking scheme, will be converted into a raster image. This is a necessary step for both display on the web and for processing by OCR engines.
* **Format Recommendation:** The recommended output format for these images is **WebP**. This modern format provides excellent compression and quality compared to older formats like `PNG` and `JPEG`, resulting in smaller file sizes and faster load times for the end-user.¹⁰ Python libraries such as `Pillow` have robust support for creating `WebP` files.¹⁰ While some PDF processing libraries like `PyMuPDF` may not write to `WebP` directly, they can output an intermediate format (like `PNG`) which can then be easily converted to `WebP` by `Pillow` in a subsequent step.¹¹
* **File Upload to Supabase Storage:** The generated `WebP` images for both questions and marking schemes are then uploaded to a designated Supabase Storage bucket. This can be automated using the Supabase Python client library, which provides functions for file uploads.¹⁴ Upon successful upload, the library returns a public URL for the file. These URLs are captured and stored, as they will be inserted into the `questions` table in the final phase of the pipeline.

### 2.2 Phase 2: Optical Character Recognition (OCR) and Coordinate Extraction

This phase is dedicated to extracting textual content from the generated question images. The primary goal is not just to get the text, but to get it in a structured format that includes positional data.

* **Text and Bounding Box Extraction:** The core requirement for this step is to use an OCR engine capable of providing both the recognized text and the bounding box coordinates for each individual word.⁸ This detailed coordinate data is the foundation of the keyword highlighting feature.
* **Tooling Considerations:** Standard OCR libraries like `easyocr` or `pytesseract` are capable of providing this word-level data.⁷ However, given that the source material contains mathematical formulas, which are a known challenge for general-purpose OCR ¹⁸, it is highly recommended to evaluate specialized models. Tools like **Nougat** and **LaTeX-OCR** are transformer-based models specifically trained on academic documents and mathematical notation.²⁰ These models are designed to understand the complex two-dimensional structure of formulas and are likely to produce significantly more accurate text and LaTeX output, which is crucial for the quality of the search index.²⁴
* **Output:** For each question image, this phase produces two key artifacts:
    1.  A single string containing the full extracted text of the question.
    2.  A JSON object that maps each word in the text to its (x, y, width, height) coordinates on the image.

### 2.3 Phase 3: AI-Powered Topic Tagging

Once the plain text of a question is available, it can be categorized into one or more predefined topics. This classification task is well-suited for a Large Language Model (LLM).

* **Methodology: Few-Shot Prompting:** Research and practical experiments consistently show that while zero-shot prompting (asking the model to classify without examples) can be unreliable, few-shot prompting dramatically improves accuracy for classification tasks.²⁷ The prompt sent to the LLM should be carefully engineered to include a few high-quality examples of exam questions and their correct topic classifications. This provides the model with the necessary context to understand the task and the desired output format.
* **Constraining Output:** A common failure mode for LLMs in classification is "inventing" new categories that are semantically similar but not identical to the predefined list (e.g., outputting "Quadratic Equations" when the official topic is "Algebra"). The prompt must include a strict instruction for the LLM to only use labels from the provided list of official topics.²⁸
* **Output:** The LLM will process the question's text and return a list of one or more topic names that apply to it (e.g., `["Algebra", "Calculus"]`).

### 2.4 Phase 4: Database Population

This final phase of the pipeline aggregates all the artifacts generated in the previous steps and populates the Supabase database.

* **Data Aggregation:** A script gathers the following for each question: the public URL for the question image, the public URL for the marking scheme image, the full extracted text, the JSON object of word coordinates, and the list of topic names from the LLM.
* **Transactional Write:** To ensure data integrity, the database insertion should be performed as a single logical transaction using the Supabase Python client. The process is as follows:
    1.  An `INSERT` statement is executed on the `questions` table, containing all the aggregated data. The database returns the UUID of the newly created question record.
    2.  The script then queries the `topics` table to retrieve the UUID for each topic name returned by the LLM.
    3.  Finally, for each associated topic, an `INSERT` statement is executed on the `question_topics` join table, creating a record that links the new `question_id` to the corresponding `topic_id`.

This structured, multi-phase pipeline ensures that by the time data reaches the database, it is clean, enriched, structured, and ready to be served efficiently to the end-user.

---

## Section 3: High-Performance Querying and User Experience

The backend's primary responsibility during a user session is to respond to filter and search requests quickly and accurately. A slow or unresponsive interface will lead to user frustration. The architecture for the live query system is therefore designed around speed, leveraging specialized database features and efficient data retrieval patterns to handle a large volume of questions. The performance of the application is not the result of one single trick, but rather the synergistic effect of the relational schema, a targeted indexing strategy, and intelligent client-side data fetching.

### 3.1 The Keyword Search Engine: PostgreSQL Full-Text Search (FTS)

To implement a powerful keyword search, the system will utilize PostgreSQL's native Full-Text Search (FTS) engine, which is fully exposed through the Supabase API.²⁹ This is a mature, feature-rich technology that far surpasses the capabilities and performance of simple string matching with the `LIKE` operator.

* **Implementation:** The core of the FTS implementation involves two key PostgreSQL objects:
    * **`tsvector`**: A special data type that stores a document's text in an optimized format for searching. It breaks the text into tokens (lexemes), removes common "stop words" (like 'the', 'a', 'an'), and applies stemming to reduce words to their root form (e.g., "solving" and "solved" both become "solve").³¹ A `tsvector` column will be added to the `questions` table and will be automatically populated from the `full_text` column using a generated column definition.
    * **`tsquery`**: This represents the user's search query, also processed into tokens and ready for matching. The Supabase client libraries handle the conversion of a user's input string into a `tsquery`.
* **The Match Operator (`@@`):** Queries are performed using the `@@` operator, which efficiently checks if a `tsquery` matches a `tsvector`.²⁹ This approach also supports advanced search syntax, allowing users to combine terms with boolean operators like `AND` (`&`) and `OR` (`|`), providing a search experience akin to a modern search engine.²⁹

### 3.2 Optimizing for Speed: A Multi-Index Strategy

Without database indexes, every query that filters by year, topic, or keyword would force PostgreSQL to perform a "full table scan"—reading every single row in the `questions` table to find matches. This is the primary cause of slow query performance in database applications and is completely avoidable with a proper indexing strategy. Indexes are data structures that allow the database to find rows matching specific criteria almost instantly.

#### B-Tree Indexes
This is the default and most common index type in PostgreSQL. It is highly efficient for equality (`=`) and range (`<`, `>`) queries.³⁵ B-Tree indexes will be created on:
* **All foreign key columns:** `subject_id` in the `questions` table, and both `question_id` and `topic_id` in the `question_topics` join table. Indexing foreign keys is a critical best practice that dramatically speeds up `JOIN` operations and referential integrity checks.³⁶
* **Frequently filtered columns:** The `year` and `question_number` columns in the `questions` table will also receive B-Tree indexes to optimize filtering by these criteria.

#### GIN (Generalized Inverted Index)
This is a specialized index type designed for indexing composite values, where a single table row can contain multiple keys. This makes it the perfect choice for Full-Text Search.³⁹ A GIN index will be created on the `fts_vector` (`tsvector`) column in the `questions` table. This index is what makes keyword searches extremely fast, even across thousands of documents.

### 3.3 Architecting the Filter Interface: Combining Filters

The Supabase JavaScript client library offers a fluent, chainable API that makes it simple to construct complex queries from the frontend. This allows the application to dynamically add filters based on user selections.

A typical query from the application will involve chaining multiple filter methods together. For example, if a user filters by year, topic, and a keyword, the client-side logic would construct a query that conceptually translates to an efficient SQL statement combining `WHERE` clauses and `JOIN`s. The Supabase client handles this translation, and the database, guided by the multi-index strategy, executes the query with high performance.⁴²

### 3.4 Solving the "500+ Question" Initial Load: Server-Side Pagination

Loading all 500+ questions and their associated data (images, text) in a single request on initial page load would result in an unacceptably long wait time and a large data transfer for the user. The solution is to implement server-side pagination.

The Supabase client library provides a `.range(from, to)` method, which directly maps to the `LIMIT` and `OFFSET` clauses in SQL. This allows the frontend to request data in manageable chunks.

* **Implementation:**
    1.  On initial page load, the frontend requests only the first N questions (e.g., `range(0, 24)` for a page size of 25). The server returns only this small subset, making the initial load feel instantaneous.
    2.  As the user scrolls down the page, the frontend detects that it is nearing the end of the loaded content and automatically triggers a new request for the next chunk (e.g., `range(25, 49)`).
* This "infinite scroll" pattern continues, loading data just-in-time. The user perceives a seamless, endless list of questions, but the application never has to handle more than a small batch of data at any one time. This same `.range()` modifier is applied to all filtered queries as well, ensuring that even searches returning hundreds of results are delivered to the client efficiently.

#### Table 3: Indexing Strategy Summary

| Table             | Column(s)                 | Index Type          | Purpose / Query Optimized                                  |
|-------------------|---------------------------|---------------------|------------------------------------------------------------|
| `questions`       | `subject_id`              | B-Tree              | Filtering by subject/level; JOINs with `subjects` table.   |
| `questions`       | `year`                    | B-Tree              | Filtering by a specific year or range of years.            |
| `questions`       | `question_number`         | B-Tree              | Filtering by a specific question number.                   |
| `questions`       | `fts_vector`              | GIN                 | High-performance keyword search using Full-Text Search.    |
| `question_topics` | `question_id`             | B-Tree              | JOINs with `questions` table; finding topics for a question. |
| `question_topics` | `topic_id`                | B-Tree              | Filtering by topic; JOINs with `topics` table.             |
| `question_topics` | (`question_id`, `topic_id`) | B-Tree (Composite)  | Enforces uniqueness and optimizes lookups on the join table. |
| `topics`          | `subject_id`              | B-Tree              | Filtering topics by subject; JOINs with `subjects` table.  |


---

## Section 4: Essential Security and Access Control

A robust security posture is not an optional feature but a fundamental requirement for any production application. The architecture must ensure that data is protected from unauthorized modification while remaining accessible to legitimate users. Supabase provides a powerful, multi-layered security model based on PostgreSQL's Row-Level Security (RLS), Storage policies, and distinct API keys for different access levels. The entire security model is predicated on a clear separation of concerns: the untrusted client-side application and the trusted server-side ingestion pipeline.

### 4.1 Public Data Access: Row-Level Security (RLS)

Since the exam questions and marking schemes are intended for public consumption, the primary goal of RLS is to make them read-only for all users, preventing any modification from the public-facing API.

* **Principle of Least Privilege:** By default, when RLS is enabled on a table, all access is denied until a policy explicitly grants it. This is a core security principle.⁴⁵
* **Policy Implementation:** For the `questions`, `topics`, `subjects`, and `question_topics` tables, a simple `SELECT` policy will be created. This policy will grant read access to both the `anon` (unauthenticated users) and `authenticated` (logged-in users) roles. The `USING` clause of this policy will simply be `true`, which effectively means "any user may read any row in this table".⁴⁶
* **Write Protection:** Critically, no `INSERT`, `UPDATE`, or `DELETE` policies will be defined for the `anon` or `authenticated` roles on these tables. Because RLS denies access by default, any attempt to modify data from the client-side application using the public API will be automatically blocked by the database, ensuring the integrity of the question bank.⁴⁸

### 4.2 Securing File Storage: Storage Policies

Supabase Storage extends the RLS concept to file objects, allowing for granular control over who can read, write, and delete files. The policies are written in SQL and operate on the `storage.objects` table.⁴⁹

* **Policy Implementation:** A policy will be created on the `storage.objects` table to allow public `SELECT` (read/download) access for all files within the designated image bucket. This ensures that the frontend application can display the question and marking scheme images.
* **Write Protection:** Similar to the database RLS, permissions for `INSERT` (upload), `UPDATE`, and `DELETE` will be restricted. These operations will only be permitted for a role with elevated privileges—the `service_role`. This ensures that new files can only be added to the system by the secure, server-side ingestion pipeline.

### 4.3 Key Management Best Practices: `anon` vs. `service_role`

Supabase provides two primary API keys that correspond to two fundamentally different security contexts. Using the correct key in the correct environment is the lynchpin of the entire security architecture.

#### The `anon` Key
This is the public, or "publishable," API key. It is designed to be safely used in client-side code, such as a JavaScript web application. This key is safe to expose only because RLS is enabled. When a request is made with the `anon` key, Supabase assigns the request either the `anon` role or, if the user is logged in, the `authenticated` role. The request is then subject to the strict RLS policies defined for those roles.⁵⁰

#### The `service_role` Key
This is a secret, administrative key that possesses full privileges. It is designed for use in trusted server environments only. Crucially, the `service_role` key bypasses all RLS policies.⁵² This key must **never, under any circumstances, be exposed in client-side code or committed to a public repository.** Its sole purpose in this architecture is to be used by the secure, server-side data ingestion pipeline (from Section 2) to write data to the database and upload files to Storage. It should be managed as a secret environment variable on the server executing the ingestion scripts.⁵⁴

This strict separation of keys creates a robust security boundary. The frontend application, which is inherently untrusted, is given a key that is fundamentally limited by RLS. The powerful, RLS-bypassing key is confined to a trusted, server-only environment, making it impossible for a malicious user to perform administrative actions from their browser.

---

## Section 5: Architectural Synthesis and Visual Blueprint

This document has detailed a comprehensive backend architecture for the exam question platform, moving from the foundational data model to the specifics of data ingestion, high-performance querying, and essential security controls. This final section synthesizes these components into a unified overview and provides a visual blueprint of the entire system flow.

### 5.1 Summary of Key Architectural Decisions

The proposed architecture is built upon a series of deliberate, interconnected decisions designed to create a system that is simple, clean, efficient, and scalable:

-   **Relational Database Model:** Rejecting a brittle JSON file approach in favor of a robust PostgreSQL relational schema in Supabase. This is the core decision that enables efficient querying, data integrity, and future scalability.
-   **Unified UUID-Based Identity:** Decoupling asset identity from filenames by using database-generated UUIDs as the canonical identifier for all entities.
-   **Offline Data Ingestion Pipeline:** Separating the computationally expensive tasks of image conversion, OCR, and AI-based tagging from the live user experience. This allows for the use of higher-quality tools and ensures the frontend application remains fast.
-   **Optimized Full-Text Search:** Leveraging PostgreSQL's native Full-Text Search capabilities with a GIN index to provide a powerful and fast keyword search experience.
-   **Multi-Index Strategy and Pagination:** Implementing a combination of B-Tree and GIN indexes to optimize all filter and join operations, and using server-side pagination to ensure fast initial page loads and responsive filtering.
-   **Layered Security Model:** Enforcing a strict security boundary through a combination of Row-Level Security (RLS), Storage policies, and the disciplined use of separate `anon` and `service_role` keys for client-side and server-side contexts, respectively.

### 5.2 Comprehensive System Diagram

The following diagram illustrates the end-to-end data flow of the entire system, connecting the offline pipeline with the live application.

#### Figure 1: End-to-End System Architecture Diagram

```mermaid
graph TD
    subgraph "Offline Ingestion Pipeline (Secure Server Environment)"
        direction LR
        A(PDF Files) --> B{Python Script};
        B --> |1. Convert to WebP| C[Image Files (.webp)];
        B --> |2. OCR & Coordinate Extraction| D[Text & Coordinates];
        B --> |3. LLM Topic Tagging| E[Topic List];
        C --> |Upload using service_role key| F[Supabase Storage];
        G[Aggregated Data] --> |Write using service_role key| H[Supabase Database];
        F --> |Get Public URL| G;
        D --> G;
        E --> G;
    end

    subgraph "Live Application (User's Browser)"
        direction TB
        I(User's Browser) <--> |HTTPS Request with anon key| J[Supabase API Gateway];
    end

    subgraph "Supabase Backend"
        direction TB
        J --> K{RLS & Storage Policies};
        K --> |Query| L[PostgreSQL DB];
        L --> |Uses Indexes (B-Tree, GIN)| M[Query Engine];
        M --> J;
    end

    H -- Contains --> L
    F -- Contains --> H(Image URLs)

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
    style H fill:#9cf,stroke:#333,stroke-width:2px
    style I fill:#9f9,stroke:#333,stroke-width:2px